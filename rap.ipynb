{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rap.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SctOIdoFpfZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "eb638a7a-ec34-407e-8415-81399b84ff18"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import os\n",
        "import time\n",
        "\n",
        "# download artist dataset\n",
        "initial = input(\"Enter the name of the artist (include spaces):\")\n",
        "name = initial.replace(\" \", \"_\")\n",
        "path_to_file = '{}_untrained.txt'.format(name)\n",
        "\n",
        "# read, then decode from py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# look at the first 250 characters in the text\n",
        "print(text[:250])\n",
        "\n",
        "# unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "\n",
        "# PROCESS AND VECOTRIZE TEXT\n",
        "\n",
        "# Creating a mapping from unique characters to indicies\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the name of the artist (include spaces):taylor swift\n",
            "Length of text: 127159 characters\n",
            "I don't like your little games\n",
            "Don't like your tilted stage\n",
            "The role you made me play\n",
            "Of the fool, no, I don't like you\n",
            "I don't like your perfect crime\n",
            "How you laugh when you lie\n",
            "You said the gun was mine\n",
            "Isn't cool, no, I don't like you (oh!)\n",
            "\n",
            "But I\n",
            "75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsUkq93Rs5c_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "e7a05c9e-d911-4835-ed11-f8f73cbbe563"
      },
      "source": [
        "# PREDICTION\n",
        "\n",
        "# create training examples and targets\n",
        "\n",
        "# So break the text into chunks of seq_length+1. For example, \n",
        "# say seq_length is 4 and our text is \"Hello\". \n",
        "# The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "# the maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epock = len(text) #seq_length+1\n",
        "\n",
        "# create training examples/targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "# batch command converts individual characters to sequences of desired size\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "\n",
        "# method to duplicate and shift it to form the input and target text\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:2], target_example[:2])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"I don't like your little games\\nDon't like your tilted stage\\nThe role you made me play\\nOf the fool, n\"\n",
            "Target data: \" don't like your little games\\nDon't like your tilted stage\\nThe role you made me play\\nOf the fool, no\"\n",
            "Step    0\n",
            "  input: 28 ('I')\n",
            "  expected output: 1 (' ')\n",
            "Step    1\n",
            "  input: 1 (' ')\n",
            "  expected output: 50 ('d')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbV4K3nEs_w2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f03de84a-45d7-4527-cc3a-6a1a2bbb5915"
      },
      "source": [
        "# CREATE TRAINING BATCHES\n",
        "\n",
        "# use tf.data to split text into manageable sequences, need to shuffle data and pack into batches beforehand\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpT2_TOKtFBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f41c057a-0057-47d2-9b24-44b5246c4a91"
      },
      "source": [
        "# BUILD MODEL\n",
        "\n",
        "# use tf.keras.Sequential to define model\n",
        "# tf.keras.layers.Embedding: input layer, map numbers of each character to vector with embedding_dim dimensions\n",
        "# tf.keras.layers.GRU: RNN with size units=rnn_units\n",
        "# tf.keras.layers.Dense: Output layer with vocab_size outputs\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
        "\n",
        "#check shape\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "#sample batch\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "sampled_indices\n",
        "\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 75) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (64, None, 256)           19200     \n",
            "_________________________________________________________________\n",
            "gru_16 (GRU)                 (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (64, None, 75)            76875     \n",
            "=================================================================\n",
            "Total params: 4,034,379\n",
            "Trainable params: 4,034,379\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Input: \n",
            " \"till have me\\nYou can see that I've been cryin'\\nBaby, you know all the right things\\nTo say\\nBut do you\"\n",
            "\n",
            "Next Char Predictions: \n",
            " 'N]K\"O[7g?E-l0D4UsQTl1ULcDoQó JrLm1Fk.F((YDYCBedR?K,euzz[pYOY6z)GO7déYX7u \"v]]2Uumb1Mrv)OzCY7zeSX3Dov'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6GsIEIVtIId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0f2a000f-00fb-465a-f055-18cab5b9bc45"
      },
      "source": [
        "# TRAIN MODEL\n",
        "\n",
        "# attach optimizer, and a loss function\n",
        "# tf.keras.losses.sparse_categorical_crossentropy loss function\n",
        "# logits = from_logits flag\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "\n",
        "# Configure training procedure\n",
        "model.compile(optimizer='adam', loss=loss, metrics= ['accuracy'])\n",
        "\n",
        "# Configure checkpoints\n",
        "\n",
        "# tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 75)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.3180876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DsXHMRatKfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f9c1081-90c9-4969-8957-cdf7489fb57b"
      },
      "source": [
        "# EXECUTE TRAINING\n",
        "\n",
        "# uncomment to delete folder\n",
        "#!rm -rf 'training_checkpoints'\n",
        "\n",
        "\n",
        "# use 10 epocks to train model, 1 epoch = period of time of a person's life\n",
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.6213 - accuracy: 0.8339\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.5732 - accuracy: 0.8515\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 1s 44ms/step - loss: 0.5291 - accuracy: 0.8667\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 0.4901 - accuracy: 0.8812\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 0.4562 - accuracy: 0.8936\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 1s 44ms/step - loss: 0.4222 - accuracy: 0.9056\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 1s 44ms/step - loss: 0.3957 - accuracy: 0.9145\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.3675 - accuracy: 0.9226\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.3472 - accuracy: 0.9293\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 0.3271 - accuracy: 0.9351\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 1s 48ms/step - loss: 0.3085 - accuracy: 0.9392\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.2960 - accuracy: 0.9427\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 0.2810 - accuracy: 0.9450\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 1s 56ms/step - loss: 0.2705 - accuracy: 0.9474\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.2598 - accuracy: 0.9500\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 1s 48ms/step - loss: 0.2536 - accuracy: 0.9503\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 1s 44ms/step - loss: 0.2459 - accuracy: 0.9524\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 1s 46ms/step - loss: 0.2393 - accuracy: 0.9530\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 1s 44ms/step - loss: 0.2332 - accuracy: 0.9539\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.2292 - accuracy: 0.9545\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.2201 - accuracy: 0.9567\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.2223 - accuracy: 0.9562\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.2133 - accuracy: 0.9575\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 1s 54ms/step - loss: 0.2122 - accuracy: 0.9576\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 1s 44ms/step - loss: 0.2100 - accuracy: 0.9586\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 1s 48ms/step - loss: 0.2024 - accuracy: 0.9593\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.1980 - accuracy: 0.9600\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.1987 - accuracy: 0.9596\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 1s 44ms/step - loss: 0.1922 - accuracy: 0.9612\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 1s 45ms/step - loss: 0.1949 - accuracy: 0.9607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMV3MMiMtMlk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "997372b8-b1c2-41c7-d94e-8bcb928385b3"
      },
      "source": [
        "# GENERATE TEXT\n",
        "\n",
        "# rebuild and restore weights from checkpoint to run with different batch_size\n",
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "#print(history)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (1, None, 256)            19200     \n",
            "_________________________________________________________________\n",
            "gru_17 (GRU)                 (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (1, None, 75)             76875     \n",
            "=================================================================\n",
            "Total params: 4,034,379\n",
            "Trainable params: 4,034,379\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Co5znpGtPWl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "342b0529-89ef-4021-9881-15264272d39c"
      },
      "source": [
        "# PREDICTION LOOP\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 2000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "# write output to single text file\n",
        "# creates outputs based on helpfulwords and combines into one file\n",
        "helpfulWords= [u\"oh \", u\"now \", u\"never \", u\"I \", u\"so \"]\n",
        "filename = '{}_trained_output.txt'.format(name)\n",
        "out_text = open(filename, \"w+\")\n",
        "for word in helpfulWords:\n",
        "    print(\"starting word: '{}' generating lyrics...-> {}\".format(word,filename))\n",
        "    out_text.write(generate_text(model, start_string = word))\n",
        "out_text.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting word: 'oh ' generating lyrics...-> taylor_swift_trained_output.txt\n",
            "starting word: 'now ' generating lyrics...-> taylor_swift_trained_output.txt\n",
            "starting word: 'never ' generating lyrics...-> taylor_swift_trained_output.txt\n",
            "starting word: 'I ' generating lyrics...-> taylor_swift_trained_output.txt\n",
            "starting word: 'so ' generating lyrics...-> taylor_swift_trained_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}